---
title: "Stat 201 - Lecture 17"
author: "Prof. Heggeseth"
date: "November 17, 2015"
output: slidy_presentation
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(dev='png',fig.path='Figs/',echo=TRUE,message=FALSE, warning=FALSE,message=FALSE,tidy=TRUE,fig.width = 6,fig.height=6)
```

--------------

#Review

If we are interesting in estimating the **proportion** of the population $p$ that has some characteristic or opinion, we know quite a bit about the behavior of sample proportions.

1. If our draws are approximately independent (Rule of Thumb: $n < 0.10N$), then the sample count of "successes" in our sample is a **Binomial RV** (which is the sum of $n$ Bernoulli RV's, $\sum_{i=1}^n X_i$) and thus,

\[ E(\hat{p}) = E\left(\frac{\sum_{i=1}^n X_i}{n}\right) = p \]
\[ SD(\hat{p}) = SD\left(\frac{\sum_{i=1}^n X_i}{n}\right) = \sqrt{\frac{p(1-p)}{n}} \]

2. If our sample size $n$ is large enough (Rule of Thumb: $np>10$ and $n(1-p)>10$), then the **sampling distribution model for sample proportions,** $\hat{p}$, is approximately Normal (by CLT).

```{r,fig.width = 10,fig.height=4}
x=330:470
y=dbinom(x,1000,0.4)
par(cex=0.6)
barplot(y,names=x/1000,xlab='Sample Proportions',main='Probabilities based on Binomial')
```

```{r,echo=FALSE,fig.width = 10,fig.height=4}
p = seq(0.2,.8,by=.01)
plot(p,dnorm(p,mean = 0.5,sd = sqrt(0.5*0.5/50)),type='l',xaxt='n',yaxt='n',bty='n',ylab='',xlab='Sample Proportions',main = 'If n is large, Sampling Distribution of Proportions\n (Normal by CLT)' ) #Theoretical Model
abline(h=0)
segments(.5,-.1,.5,.1)
mtext('p',side=1,line=.1,cex=.8)
segments(.5+sqrt(.5*.5/50),-.1,.5+sqrt(.5*.5/50),.1)
mtext('p + SD',side=1,line=.1,at=.5+sqrt(.5*.5/50),cex=.8)
segments(.5-sqrt(.5*.5/50),-.1,.5-sqrt(.5*.5/50),.1)
mtext('p - SD',side=1,line=.1,at=.5-sqrt(.5*.5/50),cex=.8)
segments(.5+2*sqrt(.5*.5/50),-.1,.5+2*sqrt(.5*.5/50),.1)
mtext('p + 2SD',side=1,line=.1,at=.5+2*sqrt(.5*.5/50),cex=.8)
segments(.5-2*sqrt(.5*.5/50),-.1,.5-2*sqrt(.5*.5/50),.1)
mtext('p - 2SD',side=1,line=.1,at=.5-2*sqrt(.5*.5/50),cex=.8)
```

--------------

#Real Life

After we take a sample and have observed data, all we have is $\hat{p}$, the sample proportion. 

- We don't know $p$, the population proportion.

--------------

#Real Life

But, we know that our $\hat{p}$ should be close to the true $p$.

- About 95% of all possible samples are within about 2 SD's, $\pm 2\sqrt{\frac{p(1-p)}{n}}$, of $p$.

Therefore, if we set a "trap" that is about 2 SD's wide, we'll "catch" the true $p$ in about 95% of the random samples. 

\[ \hat{p} \pm 2 \sqrt{\frac{p(1-p)}{n}}\]


But, we don't know $p$. 

- We estimate $SD(\hat{p}) = \sqrt{\dfrac{p(1-p)}{n}}$ by plugging in $\hat{p}$ and call it the **standard error**,

$$SE(\hat{p}) = \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$$


```{r,echo=FALSE,fig.width=10,fig.height=6}
require(mosaic)
pop <- c(rep(1,.48*100000),rep(0,100000-.48*100000))
set.seed(5)
sample_props504 <- do(100,parallel=FALSE) * sum(sample(pop,504))/504
#hist(sample_props504[,1], main='Sampling Distribution')
includ = exclud = 0
plot(rep(.48,10),seq(0,100,length=10),type='l',yaxt='n',ylab =" ",xlab='Sample Proportion',bty='n',xlim=c(0.3,0.7))
mtext('p',1,cex=.7,at=0.48)
for(i in 1:100){
  p = sample_props504[i,1]
  points(p,i,cex=.3)
  s = sqrt(p*(1-p)/504)
  segments(p-2*s,i,p+2*s,i,col=ifelse(0.48>p-2*s & 0.48< p+2*s,'green','red'))
 if(0.48>p-2*s & 0.48< p+2*s){
  includ <- includ + 1
}else{exclud <- exclud + 1}
}
abline(v=0.48)
abline(v = 0.48+2*sqrt(0.48*(1-0.48)/504),lty=2)
abline(v = 0.48-2*sqrt(0.48*(1-0.48)/504),lty=2)
mtext('p + 2SD',1,cex=.7,at=0.48+2*sqrt(0.48*(1-0.48)/504))
mtext('p - 2SD',1,cex=.7,at=0.48-2*sqrt(0.48*(1-0.48)/504))
text(0.33,90,paste('p Included:',includ),cex=.6)
text(0.33,87,paste('p Excluded:',exclud),cex=.6) 
```

------------

#Theoretical Confidence Intervals

*95% confidence interval for $p$*

$$\hat{p} \pm 1.96 \sqrt{\dfrac{\hat{p}(1-\hat{p})}{n}}$$

```{r}
qnorm(0.975) #the point at which 0.975 is below (inverse integral) and 0.025 is above
```

*99% confidence interval for $p$*

$$\hat{p} \pm 2.58 \sqrt{\dfrac{\hat{p}(1-\hat{p})}{n}}$$

```{r}
qnorm(0.995) #the point at which 0.995 is below (inverse integral) and 0.005 is above
```


----------

#Calculate the Necessary Sample Size

Suppose we want the **margin of error** (half length of "trap") to be $0.03$ in a 95% confidence interval. How large a sample do we need to take? 

$$\hat{p} \pm \text{Margin of Error}$$


----------

#Calculate the Necessary Sample Size

Suppose we want the **margin of error** (half length of "trap") to be $0.03$ in a 95% confidence interval. How large a sample do we need to take? 

We don't know $\hat{p}$ yet, so we have to supply one. Let's use $p=0.5$.

$$1.96 \sqrt{\frac{p(1-p)}{n}} = 0.03 \implies n = \frac{1.96^2 \times 0.5 \times 0.5}{0.03^2} = 1067$$

This is why most polls use $n=1000$ or a little larger.

Why did we take $p=0.5$?  

- Because this is the situation  with the highest uncertainty, where $p(1-p)$ is maximized:

```{r,fig.height=4,fig.width=6}
p=seq(0,1,len=1000)
y=p*(1-p)
plot(y~p,type="l",ylab='Variance of Bernoulli RV')
```

Should we always take $p=0.5$ when doing sample size calculations?  No!! 

- If we have a better idea of $p$, then use that!

###Example

Suppose a retail company is expecting that only 3% of the public will respond to their latest offer and wants the ME to be $0.001$. If we use $p=0.5$, 

```{r}
(1.96^2*0.5*0.5)/0.001^2
```

but if they use their expected response rate of $p=0.03$, they'd get:

```{r}
(1.96^2*0.03*0.97)/0.001^2
```

That's still large, but more reasonable as a test market size for a large company (as compared to 1,000,000).

----------

##Quantitative data 

Population proportions are numerical summaries of categorical variables.

What if we are interested in quantitative variables? We often find a mean!

We know a few things about the behavior of sample means.

1. If our draws are approximately independent ($n < 0.10N$) and identically distributed (coming from the same population) with $E(X_i) = \mu$ and $SD(X_i) = \sigma$, then

\[ E(\bar{x}) = E\left(\frac{\sum_{i=1}^n X_i}{n}\right) = \mu \]
\[ SD(\bar{x}) = SD\left(\frac{\sum_{i=1}^n X_i}{n}\right) = \frac{\sigma}{\sqrt{n}} \]

2. If our sample size $n$ is large enough (no rule of thumb, depends on how skewed the population is), then the **sampling distribution model for sample means** $\bar{x}$ is approximately Normal (by CLT).

```{r,echo=FALSE,fig.width=10,fig.height=4}
p = seq(0.2,.8,by=.01)
plot(p,dnorm(p,mean = 0.5,sd = sqrt(0.5*0.5/50)),type='l',xaxt='n',yaxt='n',bty='n',ylab='',xlab='Sample Means',main = 'If n is large, Sampling Distribution of Means\n (Normal by CLT)' ) #Theoretical Model
abline(h=0)
segments(.5,-.1,.5,.1)
mtext(expression(mu),side=1,line=.1,cex=.8)
segments(.5+sqrt(.5*.5/50),-.1,.5+sqrt(.5*.5/50),.1)
mtext(expression(paste(mu,' + SD')),side=1,line=.1,at=.5+sqrt(.5*.5/50),cex=.8)
segments(.5-sqrt(.5*.5/50),-.1,.5-sqrt(.5*.5/50),.1)
mtext(expression(paste(mu,' - SD')),side=1,line=.1,at=.5-sqrt(.5*.5/50),cex=.8)
segments(.5+2*sqrt(.5*.5/50),-.1,.5+2*sqrt(.5*.5/50),.1)
mtext(expression(paste(mu,' + 2SD')),side=1,line=.1,at=.5+2*sqrt(.5*.5/50),cex=.8)
segments(.5-2*sqrt(.5*.5/50),-.1,.5-2*sqrt(.5*.5/50),.1)
mtext(expression(paste(mu,' - 2SD')),side=1,line=.1,at=.5-2*sqrt(.5*.5/50),cex=.8)
```

###Example 

Say we are interested in average net gains from gambling at roulette, choosing red (winning $2 or losing $1).

If we play twice, then our sampling distribution for average net gain in two plays is

```{r}
require(mosaic)
p = 18/38
sim = do(1000)*mean(sample(c(-1,2),size = 2,replace=TRUE,prob=c(1-p,p)))
barplot(prop.table(table(sim[,1])),xlab='Average Net Gain')
```

If we play 10 times,

```{r}
sim = do(1000)*mean(sample(c(-1,2),size = 10,replace=TRUE,prob=c(1-p,p)))
barplot(table(sim[,1]),xlab='Average Net Gain')
```


If we play 100 times,

```{r}
sim = do(10000)*mean(sample(c(-1,2),size = 100,replace=TRUE,prob=c(1-p,p)))
barplot(table(sim[,1]),xlab='Average Net Gain')
```


What happening to the shape? center? spread?

1. Shape -- becoming Normal
2. Center -- centered at the mean (in this case $0.42)
3. Spread -- is equal to $\frac{\sigma}{\sqrt{n}}$ which for one play of the game is, $\sigma = \sqrt{2.24}$ (we often don't know $\sigma$)

```{r}
ev = -1*(1-p)+2*p
(-1 - ev)^2 *(1-p) + (2 - ev)^2*p
```

So, the sampling distribution becomes more Normal and more closely centered around $\mu$.

So, we could take

$$ \bar{x} \pm 1.96 \dfrac{\sigma}{\sqrt{n}}$$ 

as our trap for $\mu$. 

But, how did we estimate the SD for proportions? But what do we use here? 

For quantitative data, $\sigma$ is not necessarily related to $\mu$ !!!!

----------

#Gosset 

William S. Gosset and Beer

![](http://upload.wikimedia.org/wikipedia/commons/thumb/4/42/William_Sealy_Gosset.jpg/240px-William_Sealy_Gosset.jpg)

![](http://sixfiftyml.com/wp-content/uploads/2013/12/guiness.jpg)

Gosset was the quality control chemist and mathematician for Guinness Brewery in the early 1900's.

- Applied statistical knowledge to brewery and 4 experiment farms (selecting best yielding barley and hops)
- Main issue: small sample sizes!
- Learned by study, by trial and error, and by spending two terms in 1906â€“1907 in the biometrical laboratory of Karl Pearson. 

Gosset's original simulation based on characteristics of criminals (body heights)

- Split the data (3000 criminals) into samples of size 4 ($n=4$)
- Estimate the mean and sd height of each of the 750 samples 
- Look at the distribution of the *z* score heights, using the mean of the 3000 as $\mu$, but using $s$ in the denominator from each sample instead of the overall "known" $\sigma$ of the 3000:

\[ z = \frac{\bar{x} - \mu}{s/\sqrt{4}}\]

```{r,echo=FALSE,message=FALSE,result=FALSE}
require(stats)
heIn <- round(as.numeric(colnames(crimtab)) / 2.54)
dat <- data.frame(height = rep(heIn, colSums(crimtab)))
```

```{r}
dat$height = sample(dat$height,replace=FALSE)
dat$sampleId <- as.factor(rep(1:750, each = 4))

h.mean <- mean(height~sampleId, data = dat)
h.sd   <- sd(height~sampleId, data = dat)

zobs <- (h.mean - mean(dat$height))/h.sd

infZ <- is.infinite(zobs)
table(infZ) # a few infinite values (dividing by 0...)
zobs[infZ] <- 6 * sign(zobs[infZ])

hist(zobs, freq=FALSE,main='',xlab='Z-scores',breaks=50)
```

Does this look Normal?

```{r}
qqnorm(zobs)
qqline(zobs)
```

No, it is not Normal! The tails are thicker (more populous) than expected.

Why?

- $s$ is quite variable with small sample sizes, so we tend to get larger z-scores than we expect. 

------------

#Beer Helping the Field of Statistics

For the sake of making better beer, William Gosset worked out the sampling distribution of a standardized sample mean (with $s$ in the denominator),

\[t = \frac{\bar{x} - \mu}{s/\sqrt{n}}\]

if the histogram of the **population is unimodal and roughly symmetric** (approximately Normal).

William Gosset prepared his results about the sampling distribution for publication, but Guinness wouldn't allow employees to publish under their own name, so he published under the pseudonym **Student**.

Thus, this sampling distribution model that he discovered became known as **Student's model.** Later R.A. Fisher (another famous statistician) recoined it as *Student's T model.*

- The center is always 0.
- The spread is determined by a parameter called **degrees of freedom** ($df = n - 1$).
- As $n\rightarrow\infty$, becomes more Normal. 

```{r echo=FALSE,fig.width=10,fig.height=4}
x <- seq(-5,5,by=0.01)
plot(x,dt(x, df = 5), type='l',ylab=' ',xlab = 'Standardized Sample Means',col='green',ylim=c(0,0.42),lwd=2,main="Student's T Model")
abline(h=0)
lines(x,dt(x, df = 10),col ='blue',lwd=2)
lines(x,dt(x, df = 25),col ='red',lwd=2)
lines(x,dt(x, df = 100),col ='purple',lwd=2)
lines(x,dnorm(x),col ='black',lwd=2)
legend('topright',legend=c('df = 5','df = 10','df = 25', 'df = 100','Std. Normal'),lwd=2,col=c('green','blue','red','purple','black'))
```


----------

#Theoretical Confidence Interval for the Mean

So the bottom line from Gosset's experiment is that to create a $100(1-\alpha)%$ confidence interval for a mean, we find:

$$ \bar{x} \pm t^*_{n-1,\alpha/2} \frac{s}{\sqrt{n}}$$

where the **critical values**, $t^*_{n-1,\alpha/2}$, are the points $t^*$ on the Student's T model such that $\alpha/2 = \int_{t^*}^{\infty}f(x)dx = 1- F(t^*)$ where $f(x)$ is the pdf and $F(x)$ is the cdf of the Student's T model.

```{r}
qt(0.975, df = 5) #1-alpha = 0.95, alpha = 0.05, alpha/2 = 0.025, 
qt(0.975, df = 10)
qt(0.975, df = 20)
qt(0.975, df = 40)
qt(0.975, df = 100)
qt(0.975, df = 300)
```

##Example

**Alpha acid**: class of chemical compounds found in the resin glands of the flowers of the hop plant and are the source of hop bitterness.

![](hops.jpg)

What is the mean alpha acid of Centennial hops? In a sample of 10 Centennial hop flowers, the mean alpha acid % is 11.2  with an sd of 1.2. 

We are 95% confident that the true mean alpha acid of Centennial hops is within the following interval

```{r}
samp = c(10.3,12.1,11,9.7,10.5,12.7,10.9,11.3,10.2,13.3)
xbar = mean(samp)
s = sd(samp)

ll = xbar- qt(.975,df = 10-1)* s/sqrt(10)
ul = xbar + qt(.975,df = 10-1)* s/sqrt(10)
ci = c(ll,ul)
print(ci)
```

Compare to:

```{r}
t.test(samp)
```
 or 
 
```{r}
res=do(1000)*mean(resample(samp))
quantile(res[,1],c(0.025,.975))
```







